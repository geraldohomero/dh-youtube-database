import os
import sqlite3
import pandas as pd
import re
import spacy
from spacy.lang.pt.stop_words import STOP_WORDS
from pathlib import Path
import unicodedata
import gc
import psutil
import time
import sys
from typing import Generator, List, Optional

# ------------------------------------------------------------
# Configura√ß√£o de diret√≥rios
# ------------------------------------------------------------
BASE_DIR = Path(__file__).resolve().parent.parent
RAW_DB_PATH = BASE_DIR / "db" / "YouTubeStatsPipe2.sqlite3"
OUTPUT_PATH = BASE_DIR / "data" / "processed" / "transcripts_limpos4ComMetric.csv"

# Configura√ß√£o de mem√≥ria - otimizada para 32GB RAM
INITIAL_CHUNK_SIZE = 2000  # Aumentado de 500 para 2000
MIN_CHUNK_SIZE = 200       # Aumentado de 50 para 200
MAX_CHUNK_SIZE = 5000      # Aumentado de 1000 para 5000
CHUNK_SIZE = INITIAL_CHUNK_SIZE
MAX_MEMORY_PERCENT = 85    # Aumentado de 75 para 85
MEMORY_THRESHOLD = 90      # Aumentado de 80 para 90
CRITICAL_MEMORY = 95       # Aumentado de 90 para 95
MEMORY_CHECK_FREQUENCY = 50  # Reduzimos a frequ√™ncia de verifica√ß√£o

# ------------------------------------------------------------
# Fun√ß√£o para monitorar uso de mem√≥ria
# ------------------------------------------------------------
def get_memory_usage():
    """Retorna o uso atual de mem√≥ria como porcentagem."""
    # For√ßar coleta antes de verificar para obter valor mais preciso
    gc.collect()
    process = psutil.Process(os.getpid())
    return process.memory_percent()

def check_memory_usage(force_gc=False):
    """Verifica o uso de mem√≥ria e for√ßa coleta de lixo se necess√°rio."""
    mem_usage = get_memory_usage()
    
    # Sa√≠da de emerg√™ncia se atingir mem√≥ria cr√≠tica
    if mem_usage > CRITICAL_MEMORY:
        print(f"\nüö® ALERTA CR√çTICO: Uso de mem√≥ria em {mem_usage:.1f}% - Encerrando para evitar travamento!")
        sys.exit(1)
        
    # Verificar se estamos acima do limite
    if mem_usage > MAX_MEMORY_PERCENT or force_gc:
        print(f"\n‚ö†Ô∏è Uso de mem√≥ria alto: {mem_usage:.1f}% - Executando coleta de lixo...")
        
        # Coleta de lixo mais agressiva
        for _ in range(3):  # M√∫ltiplas passagens
            gc.collect()
            
        # Aguardar brevemente para o sistema liberar a mem√≥ria
        time.sleep(0.5)
        
        mem_usage_after = get_memory_usage()
        print(f"Mem√≥ria ap√≥s coleta: {mem_usage_after:.1f}%")
        
        # Se ainda estiver acima do threshold, pausa para o sistema recuperar
        if mem_usage_after > MEMORY_THRESHOLD:
            pause_time = min(5, (mem_usage_after - MEMORY_THRESHOLD) / 5)  # Pausa proporcional
            print(f"Pausando por {pause_time:.1f} segundos para recuperar mem√≥ria...")
            time.sleep(pause_time)
            
            # Verificar novamente ap√≥s a pausa
            mem_usage_final = get_memory_usage()
            print(f"Mem√≥ria ap√≥s pausa: {mem_usage_final:.1f}%")
            
            # Ajustar tamanho do chunk com base no uso de mem√≥ria
            global CHUNK_SIZE
            if mem_usage_final > MEMORY_THRESHOLD:
                # Reduzir o tamanho do chunk
                new_chunk_size = max(MIN_CHUNK_SIZE, int(CHUNK_SIZE * 0.7))
                if new_chunk_size != CHUNK_SIZE:
                    CHUNK_SIZE = new_chunk_size
                    print(f"Reduzindo tamanho do chunk para {CHUNK_SIZE}")
            elif mem_usage_final < 60:  # Mem√≥ria est√° confort√°vel
                # Aumentar o tamanho do chunk gradualmente
                new_chunk_size = min(MAX_CHUNK_SIZE, int(CHUNK_SIZE * 1.2))
                if new_chunk_size != CHUNK_SIZE:
                    CHUNK_SIZE = new_chunk_size
                    print(f"Aumentando tamanho do chunk para {CHUNK_SIZE}")
                    
        return True
    return False

# ------------------------------------------------------------
# Carregar modelo spaCy (precisa ter rodado antes no terminal:
# python -m spacy download pt_core_news_sm)
# ------------------------------------------------------------
print("Carregando modelo spaCy...")
nlp = None  # Inicializado sob demanda para economizar mem√≥ria

def get_nlp():
    """Carrega o modelo spaCy sob demanda"""
    global nlp
    if nlp is None:
        print("Inicializando modelo spaCy...")
        nlp = spacy.load("pt_core_news_sm", disable=["ner"])
    return nlp

# Paraleliza√ß√£o (podemos usar mais recursos com 32GB)
N_PROCESS = max(2, min(4, (os.cpu_count() or 1) - 1))  # Mais paralelo para 32GB

# Stopwords extras (ajuste √† vontade)
STOPWORDS_SET = set(STOP_WORDS)

# ------------------------------------------------------------
# Fun√ß√µes de limpeza
# ------------------------------------------------------------
def clean_timestamps(text: str) -> str:
    """Remove timestamps do tipo [00:10] ou [01:02:33]."""
    return re.sub(r"\[\d{1,2}:\d{2}(?::\d{2})?\]", " ", text)

def clean_urls(text: str) -> str:
    """Remove URLs simples."""
    return re.sub(r"(https?://\S+|www\.\S+)", " ", text)

def normalize_spaces(text: str) -> str:
    """Colapsa espa√ßos e trim."""
    return re.sub(r"\s+", " ", text).strip()

def text_generator(texts: List[str]) -> Generator[str, None, None]:
    """Gera textos pr√©-processados um a um para economizar mem√≥ria."""
    for text in texts:
        if not isinstance(text, str):
            yield ""
            continue
            
        # Pr√©-limpeza b√°sica
        text = text.replace("\n", " ").replace("\r", " ")
        text = clean_timestamps(text)
        text = clean_urls(text)
        text = text.lower()
        text = normalize_spaces(text)
        
        yield text

def preprocess_text_batch(texts, batch_size: int = 64, n_process: int = N_PROCESS, show_progress: bool = True):
    """Limpa e normaliza transcripts usando processamento em lotes menores."""
    # Garante iter√°vel index√°vel
    texts_list = list(texts)
    total = len(texts_list)
    results = [""] * total
    
    if total == 0:
        return results

    step = max(1, total // 20)  # atualiza a cada ~5%
    processed = 0
    
    # Processar em lotes maiores para melhor performance com mais mem√≥ria
    sub_batch_size = min(200, batch_size)  # Aumentado de 50 para 200
    
    # Pr√©-processar todos os textos de uma vez, mas armazenar em uma lista
    # para evitar problemas com o gerador
    pre_cleaned_texts = []
    for text in texts_list:
        if not isinstance(text, str):
            pre_cleaned_texts.append("")
            continue
            
        # Pr√©-limpeza b√°sica
        text = text.replace("\n", " ").replace("\r", " ")
        text = clean_timestamps(text)
        text = clean_urls(text)
        text = text.lower()
        text = normalize_spaces(text)
        pre_cleaned_texts.append(text)
    
    # Liberar mem√≥ria
    del texts_list
    check_memory_usage()
    
    # Carregar o modelo apenas quando necess√°rio
    model = get_nlp()
    
    # Processar em sub-lotes para melhor controle de mem√≥ria
    for start_idx in range(0, total, sub_batch_size):
        end_idx = min(start_idx + sub_batch_size, total)
        sub_batch_texts = pre_cleaned_texts[start_idx:end_idx]
        
        # Verifique mem√≥ria antes de processar o lote
        if processed % MEMORY_CHECK_FREQUENCY == 0:
            check_memory_usage()
        
        # Usar pipe com batch_size maior
        docs = list(model.pipe(sub_batch_texts, batch_size=min(32, len(sub_batch_texts))))  # Aumentado de 16 para 32
        
        for i, doc in enumerate(docs):
            tokens = []
            for token in doc:
                if not token.is_alpha:
                    continue
                lemma = token.lemma_.lower()
                lemma_norm = unicodedata.normalize("NFKD", lemma).encode("ascii", "ignore").decode("ascii")
                if len(lemma) <= 2:
                    continue
                if lemma in STOPWORDS_SET or lemma_norm in STOPWORDS_SET:
                    continue
                tokens.append(lemma)
            
            results[start_idx + i] = " ".join(tokens)
            
            # Liberar mem√≥ria do doc ap√≥s processar
            del doc
            
            processed += 1
            if show_progress and (processed % step == 0 or processed == total):
                pct_done = processed * 100.0 / total
                pct_left = 100.0 - pct_done
                mem_usage = get_memory_usage()
                print(f"\rProgresso: {pct_done:6.2f}% | Restante: {pct_left:6.2f}% | Mem√≥ria: {mem_usage:.1f}%", end="", flush=True)
        
        # Liberar mem√≥ria ap√≥s cada sub-lote
        del sub_batch_texts
        del docs
        
        # Verificar mem√≥ria depois de processar o lote
        if processed % MEMORY_CHECK_FREQUENCY == 0:
            check_memory_usage(force_gc=True)
    
    # Liberar mem√≥ria dos textos pr√©-processados
    del pre_cleaned_texts
    check_memory_usage()

    if show_progress:
        print()

    return results

# ------------------------------------------------------------
# Pipeline
# ------------------------------------------------------------
def process_chunk(chunk):
    """Processa um chunk de dados."""
    # Converte 'publishedAt' para datetime, tratando erros
    chunk['publishedAt'] = pd.to_datetime(chunk['publishedAt'], errors='coerce')
    
    # Verifica se as colunas de m√©tricas existem e converte para o tipo num√©rico
    engagement_metrics = ['viewCount', 'likeCount', 'commentCount']
    for metric in engagement_metrics:
        if metric in chunk.columns:
            chunk[metric] = pd.to_numeric(chunk[metric], errors='coerce')
    
    if len(chunk) == 0:
        return None
    
    print(f"Processando chunk com {len(chunk)} registros...")
    
    # Lista as colunas presentes para verifica√ß√£o
    print(f"Colunas dispon√≠veis: {chunk.columns.tolist()}")
    
    # Verifica se as m√©tricas de engajamento est√£o presentes
    metrics_present = [metric for metric in engagement_metrics if metric in chunk.columns]
    if metrics_present:
        print(f"M√©tricas de engajamento inclu√≠das: {metrics_present}")
    else:
        print("AVISO: Nenhuma m√©trica de engajamento encontrada nos dados.")
    
    # Mais otimiza√ß√µes para usar mais mem√≥ria dispon√≠vel
    chunk["cleanTranscript"] = preprocess_text_batch(
        chunk["videoTranscript"].tolist(),
        batch_size=64,
        n_process=N_PROCESS, 
        show_progress=True
    )
    
    # Remove a coluna videoTranscript para economizar espa√ßo
    chunk = chunk.drop(columns=['videoTranscript'])
    
    # Garantir que valores nulos nas m√©tricas sejam substitu√≠dos por zeros
    for metric in metrics_present:
        chunk[metric] = chunk[metric].fillna(0).astype(int)
    
    return chunk

def main():
    # Add global declaration for CHUNK_SIZE
    global CHUNK_SIZE
    
    print(f"Lendo banco em: {RAW_DB_PATH}")
    
    # Define o per√≠odo de filtro (novembro de 2022 a julho de 2023, inclusivo)
    start_date = '2022-11-01'
    end_date = '2023-08-01' # O limite superior √© exclusivo para incluir todo o dia 31/07
    
    # Criar o arquivo de sa√≠da com cabe√ßalhos
    OUTPUT_PATH.parent.mkdir(parents=True, exist_ok=True)
    
    conn = sqlite3.connect(RAW_DB_PATH)
    
    # Contar total de registros para informa√ß√£o
    count_query = "SELECT COUNT(*) FROM Videos"
    total_records = pd.read_sql_query(count_query, conn).iloc[0, 0]
    print(f"Total de v√≠deos no banco: {total_records}")
    
    # Processar em chunks
    chunks_processed = 0
    total_processed = 0
    
    # Vamos modificar a consulta para buscar apenas as colunas necess√°rias
    query = """
        SELECT
            videoId,
            channelId,
            videoTitle,
            videoTranscript,
            publishedAt,
            transcriptLanguage,
            viewCount,
            likeCount,
            commentCount
        FROM Videos
        LIMIT ? OFFSET ?
    """
    
    # Processar o primeiro chunk e criar o arquivo
    first_chunk = True
    offset = 0
    
    while True:
        # Verifica uso de mem√≥ria antes de carregar novo chunk
        check_memory_usage(force_gc=True)
        
        print(f"\nCarregando chunk {chunks_processed+1} (offset {offset}, tamanho {CHUNK_SIZE})...")
        try:
            chunk = pd.read_sql_query(query, conn, params=[CHUNK_SIZE, offset])
        except Exception as e:
            print(f"Erro ao carregar chunk: {str(e)}")
            # Tentar com um chunk menor em caso de erro
            CHUNK_SIZE = max(MIN_CHUNK_SIZE, CHUNK_SIZE // 2)
            print(f"Reduzindo para chunk de tamanho {CHUNK_SIZE} e tentando novamente...")
            chunk = pd.read_sql_query(query, conn, params=[CHUNK_SIZE, offset])
        
        if len(chunk) == 0:
            break
        
        # Verificar se o chunk √© muito grande - aumentamos o limite para 32GB
        if len(chunk) > 0 and chunk.memory_usage(deep=True).sum() / (1024**2) > 2000:  # Aumentado de 500MB para 2GB
            print("Chunk muito grande! Tentando reduzir...")
            CHUNK_SIZE = max(MIN_CHUNK_SIZE, CHUNK_SIZE // 2)
            print(f"Reduzindo para {CHUNK_SIZE} registros por chunk.")
            continue  # Tente novamente com chunk menor
            
        processed_chunk = process_chunk(chunk)
        
        # Liberar mem√≥ria do chunk original imediatamente
        del chunk
        gc.collect()
        
        if processed_chunk is not None and len(processed_chunk) > 0:
            # Filtra o DataFrame para o per√≠odo desejado AP√ìS o processamento
            mask = (processed_chunk['publishedAt'] >= start_date) & (processed_chunk['publishedAt'] < end_date)
            df_filtrado = processed_chunk[mask].copy()
            
            if len(df_filtrado) == 0:
                print("Nenhum registro no per√≠odo para este chunk.")
                del processed_chunk
                del df_filtrado
                gc.collect()
                offset += CHUNK_SIZE
                continue
            
            engagement_metrics = ['viewCount', 'likeCount', 'commentCount']
            metrics_present = [metric for metric in engagement_metrics if metric in df_filtrado.columns]
            
            if metrics_present:
                print(f"Exportando com m√©tricas de engajamento: {metrics_present}")
                # Mostrar estat√≠sticas b√°sicas
                for metric in metrics_present:
                    print(f"  - {metric}: m√©dia = {df_filtrado[metric].mean():.1f}, m√°x = {df_filtrado[metric].max()}")
            else:
                print("AVISO: Nenhuma m√©trica de engajamento ser√° exportada.")
            
            # Escrever para o arquivo em modo append
            mode = 'w' if first_chunk else 'a'
            header = first_chunk
            
            # Escrever em peda√ßos maiores para economizar opera√ß√µes de I/O
            write_chunk_size = min(500, len(df_filtrado))  # Aumentado de 100 para 500
            for i in range(0, len(df_filtrado), write_chunk_size):
                sub_df = df_filtrado.iloc[i:i+write_chunk_size]
                sub_df.to_csv(
                    OUTPUT_PATH, 
                    index=False, 
                    encoding="utf-8", 
                    mode=mode, 
                    header=header
                )
                # Apenas o primeiro sub-chunk do primeiro chunk tem header
                mode = 'a'
                header = False
                del sub_df  # Liberar mem√≥ria
                
            if first_chunk:
                first_chunk = False
            
            total_processed += len(df_filtrado)
            print(f"Salvos {len(df_filtrado)} registros no arquivo. Total: {total_processed}")
        
        chunks_processed += 1
        offset += CHUNK_SIZE
        
        # Liberar mem√≥ria do chunk processado explicitamente
        del processed_chunk
        if 'df_filtrado' in locals():
            del df_filtrado
        gc.collect()
        
        # Verificar uso de mem√≥ria e ajustar o tamanho do chunk se necess√°rio
        mem_usage = get_memory_usage()
        print(f"Uso de mem√≥ria atual: {mem_usage:.1f}%")
    
    conn.close()
    
    print(f"\n‚úÖ Processamento conclu√≠do em {chunks_processed} chunks.")
    print(f"‚úÖ Transcripts processados salvos em: {OUTPUT_PATH}")
    
    # Verificar as colunas no arquivo final
    try:
        # Ler apenas o cabe√ßalho
        csv_header = pd.read_csv(OUTPUT_PATH, nrows=0).columns.tolist()
        print(f"Colunas no arquivo CSV final: {csv_header}")
        
        # Verificar m√©tricas no arquivo final
        final_metrics = [m for m in engagement_metrics if m in csv_header]
        if final_metrics:
            print(f"‚úÖ M√©tricas de engajamento inclu√≠das no arquivo final: {final_metrics}")
        else:
            print("‚ùå ERRO: Nenhuma m√©trica de engajamento encontrada no arquivo final!")
    except Exception as e:
        print(f"Erro ao verificar arquivo final: {str(e)}")
    
    print(f"Total de tokens removidos como stopwords: {len(STOPWORDS_SET)}")
    print(f"Total de registros processados: {total_processed}")

# ------------------------------------------------------------
if __name__ == "__main__":
    # Verificar depend√™ncias
    try:
        import psutil
    except ImportError:
        print("Instalando depend√™ncia psutil...")
        import subprocess
        subprocess.check_call(["pip", "install", "psutil"])
        import psutil
    
    # Verificar recursos dispon√≠veis
    mem_available = psutil.virtual_memory().available / (1024**3)
    mem_total = psutil.virtual_memory().total / (1024**3)
    print(f"Mem√≥ria do sistema: {mem_total:.2f} GB total, {mem_available:.2f} GB dispon√≠vel")
    
    # Ajustar o tamanho do chunk com base na mem√≥ria dispon√≠vel
    if mem_available < 8:  # Menos de 8GB dispon√≠vel (era 2GB)
        INITIAL_CHUNK_SIZE = MIN_CHUNK_SIZE
        CHUNK_SIZE = INITIAL_CHUNK_SIZE
        print(f"Pouca mem√≥ria dispon√≠vel! Utilizando chunks pequenos: {CHUNK_SIZE}")
    elif mem_available > 16:  # Se tiver mais de 16GB dispon√≠vel, usar chunks maiores
        INITIAL_CHUNK_SIZE = 3000
        CHUNK_SIZE = INITIAL_CHUNK_SIZE
        print(f"Muita mem√≥ria dispon√≠vel! Utilizando chunks grandes: {CHUNK_SIZE}")
    
    main()
